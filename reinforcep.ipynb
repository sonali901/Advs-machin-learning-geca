{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-659f68ec18fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from googletrans import Translator\n",
    "\n",
    "class CustomPreProcessing(object):\n",
    "    '''\n",
    "    Class to preprocess specific mails and extract informations.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        #print('''Welcome in this custom preprocessing class\n",
    "        #''')\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_features(self, df, column):\n",
    "        '''\n",
    "        Function to extract information and generate dataframe columns. The goal is to extract\n",
    "        mail informations as: Id of the courriel, the sender, the receiver, the subject, the date of sending\n",
    "        @param df: (pandas dataframe) dataframe containing all the data\n",
    "        @param column: (str) the column containing the mails\n",
    "        @return: (pandas dataframe) dataframe with the original text, the text without the header and columns containing information\n",
    "        '''\n",
    "        # ---- make some empty lists\n",
    "        text_ = []\n",
    "        sender_ = []\n",
    "        dest_ = []\n",
    "        subject_ = []\n",
    "        date_ = []\n",
    "        id_mail_ = []\n",
    "        phone_ = []\n",
    "        # ---- Save the original text\n",
    "        df.loc[:, column+\"_brut\"] = df.loc[:,column]\n",
    "        # ---- For each text the loop will extract informations, store them into lists and delete the corresponding text\n",
    "        for text in tqdm(df[column]):\n",
    "            _text = text.replace(\"\\t\", \" \").split(\"\\n\")\n",
    "            text = []\n",
    "            # ---- Clean extra space \n",
    "            for line in _text:\n",
    "                text.append(self.remove_whitespace(line))\n",
    "            sender = []\n",
    "            dest = []\n",
    "            subject = []\n",
    "            date = []\n",
    "            ind = []\n",
    "            id_mail = []\n",
    "            for i, lines in enumerate(text):\n",
    "                 \n",
    "                if any(x in lines for x in [\"Id Courriel\"]):                       # ---- Looking for Id \n",
    "                    _text = lines.split(\":\")\n",
    "                    if len(_text)>1:\n",
    "                        id_mail.append(' '.join(_text[1:]))\n",
    "                    else:\n",
    "                        id_mail.append(_text[1])\n",
    "                    ind.append(i)\n",
    "\n",
    "                if any(x in lines for x in [\"De:\",\"De :\", \"From:\", \"From :\"]):     # ---- Looking for sender\n",
    "                    _text = lines.split(\":\")\n",
    "                    if len(_text)>1:\n",
    "                        sender.append(' '.join(_text[1:]))\n",
    "                    else:\n",
    "                        sender.append(_text[1])\n",
    "                    ind.append(i)\n",
    "                    \n",
    "                if  any(x in lines for x in [\"À :\", \"À:\",\"à:\", \"à :\", \"To:\", \\\n",
    "                                             \"To :\",\"to:\", \"to :\", \"CC:\", \"CC :\", \\\n",
    "                                             \"cc:\", \"cc :\"]):                       # ---- Looking for receiver\n",
    "                    _text = lines.split(\":\")\n",
    "                    if len(_text)>1:\n",
    "                        dest.append(' '.join(_text[1:]))\n",
    "                    else:\n",
    "                        dest.append(_text[1])\n",
    "                    ind.append(i)\n",
    "                    \n",
    "                if  any(x in lines for x in [\"Subject\", \"Objet\", \"objet:\", \\\n",
    "                                             \"objet :\"]) :                          # ---- Looking for the subject\n",
    "                    _text = lines.split(\":\")\n",
    "                    if len(_text)>1:\n",
    "                        subject.append(' '.join(_text[1:]))\n",
    "                    else:\n",
    "                        try:\n",
    "                            subject.append(_text[1])\n",
    "                        except:\n",
    "                            subject.append(np.nan)\n",
    "                    ind.append(i)\n",
    "                    \n",
    "                if  any(x in lines for x in [\"Envoyé:\", \"Envoyé :\", \"Envoyé le \", \\\n",
    "                                             \"Date:\", \"Date :\"]):                    # ---- Looking for sending date\n",
    "                    _text = lines.split(\":\")\n",
    "                    if len(_text)>1:\n",
    "                        date.append(':'.join(_text[1:]))\n",
    "                    else:\n",
    "                        date.append(_text[1])\n",
    "                    ind.append(i)   \n",
    "            \n",
    "            # ---- If there is information to delete inside the text\n",
    "            if ind:\n",
    "                try:\n",
    "                    for i in ind[::-1]:\n",
    "                        del text[i]\n",
    "                except:\n",
    "                    pass \n",
    "                \n",
    "            text = '\\n'.join(map(str, text))\n",
    "            # ---- Remove phone number\n",
    "            text, phone = self.remove_phone_number(text) \n",
    "\n",
    "            # ---- Check if some lists are empty\n",
    "            if not phone: phone.append(np.nan)\n",
    "            if not id_mail: id_mail.append(np.nan)\n",
    "            if not sender: sender.append(np.nan)\n",
    "            if not dest: dest.append(np.nan)\n",
    "            if not date: subject.append(np.nan)\n",
    "\n",
    "            # ---- Stack the different informations \n",
    "            phone_.append(','.join(map(str, phone)))\n",
    "            text_.append(text )\n",
    "            sender_.append(' '.join(map(str, sender)))\n",
    "            dest_.append(','.join(map(str, dest)))\n",
    "            subject_.append(','.join(map(str, subject )))\n",
    "            date_.append(','.join(map(str, date )))\n",
    "            id_mail_.append(','.join(map(str, id_mail)))\n",
    "        \n",
    "        # ---- Construct the different columns \n",
    "        df.loc[:,column] = text_\n",
    "        df.loc[:, \"id_mail\"] =  id_mail_\n",
    "        df.loc[:, \"From\"] = sender_\n",
    "        df.loc[:, \"To\"] = dest_\n",
    "        df.loc[:, \"Subject\"] = subject_\n",
    "        df.loc[:, \"Date\"] = date_\n",
    "        df.loc[:, \"Phone\"] = phone_\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_whitespace(self, text):\n",
    "        \"\"\"\n",
    "        Function to remove extra whitespaces from text\n",
    "        @param text: (str) text\n",
    "        @return: (str) text clean from extra space\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_phone_number(self, x):\n",
    "        '''\n",
    "        Function to find and remove phone number with regex    \n",
    "        @param x: (str) text \n",
    "        @return: (str) text without phone number\n",
    "        '''\n",
    "        # ---- Phone number \n",
    "        phone = []\n",
    "        r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})') \n",
    "        for i in r.findall(x):\n",
    "            phone.append(i)\n",
    "            x = x.replace(i, ' ')\n",
    "        return x , phone\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_upper_case(self, text):\n",
    "        '''\n",
    "        Function to transform upper string in title words\n",
    "        @param text: (str) text \n",
    "        @return: (str) text without upper words \n",
    "        '''\n",
    "        sentences = text.split(\"\\n\")\n",
    "        new_sentences = []\n",
    "        for i in sentences:\n",
    "            words = text.split()\n",
    "            stripped = [w.title() if w.isupper() else w for w in words]\n",
    "            new_sentences.append(\" \".join(stripped))\n",
    "        return \"\\n\".join(new_sentences)\n",
    "    \n",
    "    @classmethod\n",
    "    def strip_text(self, text, sentence):\n",
    "        '''\n",
    "        Function to cut the text where the sentence is find and return the first part.\n",
    "        Powerful to split the text from signature.\n",
    "        @param text: (str) text\n",
    "        @param sentence: (str) beginning of the signature\n",
    "        @return: (str) first part of the text without the signature\n",
    "        '''\n",
    "        if sentence in text:\n",
    "            text = text.split(sentence)[0]\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def find_corres(self, text, list_words):\n",
    "        '''\n",
    "        Function to locate a word or list of words in a string.\n",
    "        @param text: (str) text\n",
    "        @param list_words: (str or list) word or list of words to find in the text\n",
    "        @return: (bool) indicate if the text contained the list\n",
    "        '''\n",
    "        if type(list_words)==str:     # change string in list if one word is passed\n",
    "            list_words=[list_words]\n",
    "            \n",
    "        results = []\n",
    "        for i in list_words:\n",
    "            if i in text:\n",
    "                results.append(True)\n",
    "            else:\n",
    "                results.append(False)\n",
    "        return True if sum(results)>0 else False\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_string(self, text, list_sentences):\n",
    "        '''\n",
    "        Function\n",
    "        @param text: (str) text\n",
    "        @param list_sentences: (list) list of sentences to be delete\n",
    "        @return: (str) text without a list of sentences\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        for i in list_sentences:\n",
    "            if text.find(i.lower()) != -1:           # ---- Looking for the substring \n",
    "                text = text.replace(i.lower(), ' ')\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def find(self, text, list_words):\n",
    "        _index = []\n",
    "        _text = text.lower().split()\n",
    "\n",
    "        for i in list_words:\n",
    "            idx = []\n",
    "            find = True\n",
    "            while find:\n",
    "                try:\n",
    "                    if idx:\n",
    "                        idx.append(_text.index(i.lower(), idx[-1]+1))\n",
    "                    else:\n",
    "                        idx.append(_text.index(i.lower()))\n",
    "                except:\n",
    "                    find=False\n",
    "                    _index.append(idx)\n",
    "        return _index   \n",
    "\n",
    "    @classmethod\n",
    "    def find_nearest(self, array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return array[idx]\n",
    "\n",
    "    @classmethod\n",
    "    def compare_distance_word(self, index, list_ref):\n",
    "\n",
    "        res = [[self.find_nearest( j, i) for j in index ] for i in list_ref] # find nearest value taking the smallest list into account\n",
    "        idx = [all((t - s)<=5 for s, t in zip(i, i[1:])) for i in res]  # check if the each is seperate by maximum 5 words\n",
    "        res = [x for x, y in zip(res, idx) if y == True]                # select lists where each words are closed\n",
    "        return res\n",
    "    \n",
    "    @classmethod\n",
    "    def find_words(self, text, list_words):\n",
    "        '''\n",
    "        Function to locate words in text, estime if the words are closed and if so delete them.\n",
    "        @param text: (str) text\n",
    "        @param words_list: (str or list) word or list of words to locate and delete\n",
    "        @return: (str) text with or without the list (words can't be seperated by more than 5 words)\n",
    "        '''\n",
    "        if type(list_words)==str:     # change string in list if one word is passed\n",
    "            list_words=[list_words]\n",
    "        if type(text)==tuple:\n",
    "            print(text)\n",
    "        if  all(x.lower() in text.lower() for x in list_words ): # ---- Looking for the words in the text \n",
    "            _index = self.find(text, list_words)\n",
    "            min_list = np.argmin([len(i) for i in _index])     # list index which minimal size \n",
    "            list_ref = _index[min_list]\n",
    "            result = self.compare_distance_word(_index, list_ref)\n",
    "            _text = text.split()\n",
    "            for i in result[::-1]:\n",
    "                del _text[i[0]:i[-1]+1]\n",
    "                \n",
    "            if _text:\n",
    "                return ' '.join(_text)   # return text without lower char \n",
    "            else: \n",
    "                return np.nan\n",
    "        else:\n",
    "            return text \n",
    "        \n",
    "    '''def remove_sentences(text, list_words):\n",
    "        if type(list_words)==str:\n",
    "            list_words = [list_words]\n",
    "        _index = find(text, list_words)\n",
    "        min_list = np.argmin([len(i) for i in _index])     # list index which minimal size \n",
    "        list_ref = _index[min_list]\n",
    "        result = compare_distance_word(_index, list_ref)\n",
    "        _text = text.split()\n",
    "        for i in result[::-1]:\n",
    "            del _text[i[0]:i[-1]+1]\n",
    "        return \" \".join(_text)'''\n",
    "        \n",
    "class PreProcessing(object):\n",
    "    '''\n",
    "    Class to preprocess text\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        #print(\"Welcome in the preprocessing\")\n",
    "        pass\n",
    "    \n",
    "    @classmethod    \n",
    "    def detect_lang_google(self, x):\n",
    "        '''\n",
    "        Function to detect the language of the string\n",
    "        @param x: (str) sentences of text to detect language\n",
    "        @return: (str or nan) language of the sentence\n",
    "        '''\n",
    "        translate = Translator()\n",
    "        try:\n",
    "            return translate.detect(x).lang\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_numbers(self, text):\n",
    "        '''\n",
    "        Function to remove number in text.\n",
    "        @param text: (str) sentence\n",
    "        @return: (str) clean text\n",
    "        '''\n",
    "        text = ''.join([i for i in text if not i.isdigit()])         \n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_URL(self, text):\n",
    "        '''\n",
    "        Function to remove url from text.\n",
    "        @param text: (str) sentence\n",
    "        @return: (str) clean text\n",
    "        \n",
    "        '''\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'',text)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_html(self, text):\n",
    "        '''\n",
    "        Function regex to clean text from html balises.\n",
    "        @param text: (str) sentence \n",
    "        @return: (str) clean text \n",
    "        '''\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'',text)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def remove_emoji(self, text):\n",
    "        '''\n",
    "        Function to remove emojis, symbols and pictograms etc from text\n",
    "        @param text: (str) sentences \n",
    "        @return: (str) clean text \n",
    "        '''\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def preprocess(self, text):\n",
    "        '''\n",
    "        Function to remove special characters\n",
    "        @param text: (pandas dataframe) text\n",
    "        @return: (pandas dataframe) clean text \n",
    "        '''\n",
    "        text = text.replace(\"(<br/>)\", \"\")\n",
    "        text = text.replace('(<a).*(>).*(</a>)', '')\n",
    "        text = text.replace('(&amp)', '')\n",
    "        text = text.replace('(&gt)', '')\n",
    "        text = text.replace('(&lt)', '')\n",
    "        text = text.replace('(\\xa0)', ' ')  \n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"\\x92\", \"'\")\n",
    "        return text\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_char_specific(self, text):\n",
    "        '''\n",
    "        Function to remove specific characters\n",
    "        @param text: (str) text\n",
    "        @return: (str) text without specific characters\n",
    "        '''\n",
    "        table = '!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~•'\n",
    "        table = str.maketrans(' ', ' ', table)\n",
    "        words = text.split()\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        return ' '.join(stripped)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_upper_case(self, text):\n",
    "        '''\n",
    "        Function to transform upper string in title words\n",
    "        @param text: (str) text \n",
    "        @return: (str) text without upper words \n",
    "        '''\n",
    "        words = text.split()\n",
    "        stripped = [w.title() if w.isupper() else w for w in words]\n",
    "        return \" \".join(stripped)\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_stop_words(self, x, stop_word):\n",
    "        '''\n",
    "        Function to remove a list of words\n",
    "        @param x : (str) text \n",
    "        @param stop_word: (list) list of stopwords to delete \n",
    "        @return: (str) new string without stopwords \n",
    "        '''\n",
    "        x_new = text_to_word_sequence(x)    # tokenize text \n",
    "        x_ = []\n",
    "        for i in x_new:\n",
    "            if i not in stop_word:\n",
    "                x_.append(i)\n",
    "        return \" \".join(x_)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_words(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent unigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer().fit(corpus)             # bag of words\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0)  \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_words_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent unigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(stop_words = \"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_bigram(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent bigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus) \n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_bigram_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent bigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_trigram(self, corpus, n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "\n",
    "    @classmethod\n",
    "    def get_top_n_trigram_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(3, 3), stop_words=\"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_top_n_5grams_sw(self, corpus, stop_word=None, lang=\"fr\", n=None):\n",
    "        '''\n",
    "        Function to return a list of most frequent trigrams in documents\n",
    "        @param corpus: (str or pandas.dataframe) documents \n",
    "        @param stop_word: (list) list containing stopwords\n",
    "        @param lang: (str) language of the text\n",
    "        @param n: (int) number of most frequent unigrams\n",
    "        @return: (list) most frequent unigrams\n",
    "        '''\n",
    "        if lang==\"fr\":\n",
    "            corpus = corpus.apply(lambda x: self.remove_stop_words(x, stop_word))\n",
    "        vec = CountVectorizer(ngram_range=(5, 5), stop_words=\"english\").fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
